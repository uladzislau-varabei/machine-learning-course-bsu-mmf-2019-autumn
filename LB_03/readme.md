# Лабораторная работа №3: Cвёрточные нейронные сети (CNN)

В данной работе вы будете работать с датасетом `CIFAR-10`. Ваша задача научиться предсказывать класс изображений. Целевая метрика — **accuracy**.

Данную работу на выбор можно выполнять либо с использованием `Tensorflow` 
(для установки в `Anaconda Prompt` выполнить `conda install -c conda-forge tensorflow` (версия 1.13.2), 
для [версии 2.0](https://www.tensorflow.org/guide/effective_tf2) рекомендуется следовать инструкциям по ссылке
[Tensorflow 2.0 on Anaconda install guide](https://medium.com/@shaolinkhoa/install-tensorflow-gpu-2-0-alpha-on-anaconda-for-windows-10-ubuntu-ced099010b21)),
либо `PyTorch` (для установки в `Anaconda Prompt` выполнить `conda install -c pytorch pytorch`).

Для аугментаций рекомендуется использовать пакет `Albumentations` — https://github.com/albu/albumentations#conda

1. Скачайте версию данных для Python: https://www.cs.toronto.edu/~kriz/cifar.html
2. Загрузите данные и сохраните изображения в png-файлы, предварительно изменив размеры массива на (32, 32, 3) для каждого конкретного примера. Также создайте отдельный словарь 
вида `{filename:label}` и сохраните его в виде json-файла (или создайте DataFrame и сохраните его в csv-файл).
3. Загрузите и отобразите несколько из созданных вами изображений. Сколько примеров доступно для каждого класса в тренировочном множестве?
4. Разделите файлы для обучения на два множества: тренировочное (80%) и валидационное (20%). Для этого используйте `sklearn.model_selection.train_test_split` с фиксированным 
параметром `random_state`.
5. Закодируйте классы изображения one-hot векторами (например, при помощи `keras.utils.to_categorical` или `sklearn.preprocessing.OneHotEncoder`).
6. Напишите генератор, который на входе принимает список файлов и размер батча, а на выходе выдает батч из загруженных изображений заданного размера и one-hot вектора, 
соответствующие их классам.
7. Напишите функцию, которая будет случайным образом аугментировать батч изображений. Покажите примеры ее работы. В генератор из предыдущего пункта добавьте параметр, 
отвечающий за необходимость аугментирования изображений.
8. Создайте модель нейронной сети, которая состоит из двух свёрточных блоков, за которыми идут несколько полносвязных слоёв с батч-нормализацией и дропаутом.
    * Свёрточный блок должен иметь вид: `Conv2D -> Conv2D -> MaxPooling2D`
    * Для свёрточных слоев используйте число фильтров равное 32/64/128/256
    * Выход последнего свёрточного слоя необходимо распрямить в вектор
    * Для полносвязных слоев используйте 64-512 нейронов
    * Рекомендуется использовать 1-3 полносвязных слоя
    * Для свёрточных и полносвязных слоёв можно, например, начать с функции активации `relu`
    * На выходе у сети должен быть softmax-слой, у которого количество нейронов равно количеству классов
9. Задайте функцию потерь и оптимизатор модели.
10. Обучите модель, используя генеретор, написанный в 6-м задании (изображения из валидационного набора аугментировать не нужно). 
11. Постройте совместные графики метрики и функции потерь для тренировочных и валидационных данных.
12. Предскажите результаты для тестового множества и вычислите значений целевой метрики. Какое качество показывает модель на разных классах?
13. Поэкспериментируйте с архитектурой сети и процессом её обучения:
    * Замените `MaxPooling2D` в сверточных блоках на `AveragePooling2D`. Как изменился результат/время обучения?
    * Обучите сеть без аугментации изображений. Как сильно она влияет на результат? 
    * Измените количество нейронов в свёрточных слоях. Лучше их постепенно увеличивать/уменьшать?
    * Поэкспериментируйте с параметрами свёрточных слоев
    * Попробуйте другие функции активации в свёрточных/полносвязных слоях
    * Уменьшите/увеличите количество полносвязных слоев, как это влияет на качество?
    * Увеличивается ли точность сети при добавлении батч-нормализации/дропаута в свёрточные блоки? Насколько это замедляет процесс обучения?
14. Опишите модель, которая показала лучший результат.
15. Отправьте отчёт преподавателю.

**Срок сдачи работы: `06 ноября 2019`.**