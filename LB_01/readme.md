# Внимание! Условие работы будет изменено/дополнено
# Лабораторная работа №1


В данной лабораторной работе нужно будет построить несколько моделей для определения времени, за которое животные могут забрать новые хозяева: на основе набора 
числовых и категориальных признаков, а также текста профиля животного. 

За основу взяты данные с [соревнования на Kaggle](https://www.kaggle.com/c/petfinder-adoption-prediction/data), 
там же приведено их описание.

Целевая переменная — `AdoptionSpeed`.

Метрика качества — `TBD`.

##### Установка дополнительных пакетов

Для установки `LightGBM` необходимо запустить `Anaconda Prompt` от имени администратора и выполнить команду `conda install -c conda-forge lightgbm`.

[Инструкции по установке](https://stackoverflow.com/questions/33749735/how-to-install-xgboost-package-in-python-windows-platform) XGBoost для Windows (см. ответ `brettlyman`).


### 1. Подготовка

1. Скачайте [архив с данными](https://drive.google.com/file/d/1B3ZOIksQoIOWVRRJocPtqfI0XBvFI8LT/view?usp=sharingg).
2. Проведите разведочный анализ данных: постройте гистограммы числовых признаков, найдите наиболе популярные значения для категориальных признаков, посмотрите статистику по тексту профиля и др.
3. Разбейте исходеые данные на матрицу признаков и целевую переменную.

### 2. Модели на основе числовых и категориальных признаков

1. Постройте новые числовые признаки на основе категориальных (рекомендуется использовать функцию `pd.get_dummies()`).

`Замечание: для категориальных признаков, которые содержат большое количество значений, может иметь смысл использовать только наиболее популярные значения`

2. Создайте матрицу, на которой будет обучаться модель: для этого объедините числовые признаки в исходных данных, а также признаки, построенные в предыдущем пункте.
3. Подумайте, какие признаки можно дополнительно использовать для обучения модели, и при необходимости добавьте их к матрице признаков.
4. Стандартизируйте данных при помощи метода `sklearn.preprocessing.StandardScaler`.
5. При помощи кросс-валидации оцените работу следующих моделей:

* `LogisticRegression`
* `SVC (SVM)`
* `RandomForest`
* `XGBoost/LightGBM`

Результаты прокомментируйте. Для проведения кросс-валидации рекомендуется использовать `sklearn.model_selection.cross_val_score` (обратите внимание на параметры функции).

6. Работа каких алгоритмов зависит от стандартизации данных?
7. Для древесных алгоритмов постройте графики важности признаков (см. свойство `feature_importances_` у обученных моделей).
8. Постарайтесь улучшить результат модели, которая показала себя лучше остальных при кросс-валидации, например, при помощи подбора гиперпараметров, создания/удаления признаков и др. 
9. `...`

### 3. Модель на основе на тексте профиля

TBD

### 4. Построение прогнозов

Используя результаты предыдущих пунктов, предскажите значение целевой переменной для тестовых и данных. Например, вы можете усреднить предсказание моделей, построенных ранее, или использовать
предсказание на тексте как еще один числовой признак и обучить одну модель (имейте ввиду, что в таком случае модель должна делать предсказания на данных, которые не использовались в процессе
обучения: удобнее это сделать при помощи функции `sklearn.model_selection.cross_val_predict`).  Сохраните результат в файле `submission_ФамилияИмя.csv` и отправьте отчет преподавателю.


__Ваша итоговая оценка будет зависеть от качества построенных моделей.__


## Перед выполнением работы рекомендуется ознакомиться со следующими материалами:
1. [Python Numpy Tutorial](http://cs231n.github.io/python-numpy-tutorial/)
2. [Numpy quickstart](https://docs.scipy.org/doc/numpy/user/quickstart.html)
3. [Matplotlib tutorial](https://matplotlib.org/users/pyplot_tutorial.html)
4. [Python Plotting With Matplotlib](https://realpython.com/python-matplotlib-guide/)
5. [Pandas tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)
6. [SKLearn: Working With Text Data](https://scikit-learn.org/0.21/tutorial/text_analytics/working_with_text_data.html)

__Срок сдачи работы: TBD.__
